<!DOCTYPE HTML>
<html lang="en">
<head>
  <meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
  <title>Siyuan Yan - Medical AI Researcher</title>
  
  <meta name="author" content="Siyuan Yan">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <meta name="description" content="Siyuan Yan - Research Fellow at Monash University, specializing in Medical AI, Foundation Models, and Computational Medicine">
  
  <link rel="stylesheet" type="text/css" href="stylesheet.css">
  <link href="https://fonts.googleapis.com/css?family=Lato:400,700,400italic,700italic" rel="stylesheet" type="text/css">
  
  <script async src="https://www.googletagmanager.com/gtag/js?id=G-W84Q98NV85"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());
    gtag('config', 'G-W84Q98NV85');
  </script>

  <script type='text/javascript' src='https://d1bxh8uas1mnw7.cloudfront.net/assets/embed.js'></script>

  <style>
  .paper-title-link {
    text-decoration: none;
    cursor: pointer;
  }
  .paper-title-link:hover papertitle {
    text-decoration: underline;
  }
  .pub-toggle-btn {
    display: inline-block;
    padding: 8px 16px;
    margin-right: 10px;
    background-color: #f0f0f0;
    border: 1px solid #ddd;
    border-radius: 5px;
    cursor: pointer;
    font-size: 14px;
    transition: all 0.3s;
  }
  .pub-toggle-btn:hover {
    background-color: #e0e0e0;
  }
  .pub-toggle-btn.active {
    background-color: #1d4ed8;
    color: white;
    border-color: #1d4ed8;
  }
.paper-entry {
    display: none;
  }
.paper-entry.show {
    display: table-row;
  }lay: none;  /* 新增hide类来隐藏 */
  }
  </style>
</head>

<body>
  <nav class="navbar">
    <div class="nav-container">
      <a href="#" class="nav-name">Siyuan Yan</a>
      <div class="nav-links">
        <a href="#home">Home</a>
        <a href="#research">Research</a>
        <a href="#publications">Publications</a>
        <a href="#news">News</a>
        <a href="#awards">Honors</a>
        <a href="#press">Press</a>
      </div>
    </div>
  </nav>

  <div class="hero-section">
    <div class="hero-overlay">
      <h1 class="hero-title">Building Clinically Grounded Medical AI at Scale</h1>
      <p class="hero-subtitle">Foundation Models, Data Infrastructure & Agentic Systems for Clinical Translation</p>
    </div>
  </div>

<table style="width:100%;max-width:1000px;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
  <tbody>
    <tr style="padding:0px">
      <td style="padding:0px">
        
        <table id="home" style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
          <tbody>
            <tr style="padding:0px">
              <td style="padding:15px;width:68%;vertical-align:middle">
                <p>
                  I am a Research Fellow at Monash University's <a href="https://www.monash.edu/it/aimh-lab">AIM for Health Lab</a>, advised by <a href="https://zongyuange.github.io/">Prof. Zongyuan Ge</a>. I develop scalable medical AI ecosystems, dedicated to engineering the next generation of clinically grounded AI.
                </p>
                <p>
                  My work has been recognized by the Australian Museum Eureka Prize (2024), Victorian Education Award (2025), and Victorian Melanoma Researcher of the Year (2025), and has led to real-world clinical deployment through the <a href="https://acemid.centre.uq.edu.au/">ACEMID screening network</a> and the NHMRC-funded national clinical trial.
                </p>
                
                <p style="margin-top:20px;">
                  <strong>I am open to collaborations and student supervision in Medical AI. Specifically, fully funded PhD positions are available at the <a href="https://www.monash.edu/it/aimh-lab">AIM for Health Lab</a>. Please reach out.</strong>
                </p>

                <p style="text-align:center; margin-top:25px;">
                  <a href="mailto:siyuan.yan@monash.edu">Email</a> &nbsp/&nbsp
                  <a href="https://scholar.google.com/citations?user=LGcOLREAAAAJ&hl=en">Google Scholar</a> &nbsp/&nbsp
                  <a href="https://github.com/SiyuanYan1">Github</a> &nbsp/&nbsp
                  <a href="https://www.linkedin.com/in/siyuan-yan-1496671bb/">LinkedIn</a>
                </p>
              </td>

              <td style="padding:15px;width:32%;max-width:32%;text-align:center;vertical-align:middle">
                <a href="icon/me3.png"><img style="width:100%;height:270px;object-fit:cover;max-width:100%;border-radius:50%" alt="profile photo" src="icon/me3.png" class="hoverZoomLink"></a>
              </td>
            </tr>
          </tbody>
        </table>

        <table id="research" style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
          <tbody>
            <tr>
              <td style="padding:15px;width:100%;vertical-align:middle">
                <heading>Research Focus</heading>
                <p>
                 My research advances foundation models, agentic workflows, and open data infrastructure to build collaborative and trustworthy systems for clinical translation:
                </p>
                <ul>
                  <li>
                    <strong>Foundation Models, MLLMs & Agentic AI:</strong> Developing general-purpose medical AI systems for collaborative clinical decision-making, as in <a href="https://www.nature.com/articles/s41591-025-03747-y"><strong>PanDerm</strong></a> (<em>Nature Medicine</em>), <a href="#"><strong>PanDerm-2</strong></a> (<em>Preprint</em>), <a href="https://arxiv.org/abs/2601.01868"><strong>DermoGPT</strong></a> (<em>Preprint</em>), and <a href="https://arxiv.org/abs/2512.03445"><strong>MAGEN</strong></a> (<em>Preprint</em>).
                  </li>
                  <li>
                    <strong>Data Infrastructure & Open Science:</strong> Establishing data infrastructure and benchmarking standards to accelerate community-wide medical AI progress, as in <a href="https://openaccess.thecvf.com/content/ICCV2025/html/Yan_Derm1M_A_Million-scale_Vision-Language_Dataset_Aligned_with_Clinical_Ontology_Knowledge_ICCV_2025_paper.html"><strong>Derm1M</strong></a> (<em>ICCV Highlight</em>), <a href="https://arxiv.org/abs/2601.01868"><strong>DermoGPT</strong></a> (<em>Preprint</em>), <a href="https://proceedings.neurips.cc/paper_files/paper/2023/hash/3a48b0eaba26ba862220a307a9edb0bb-Abstract-Datasets_and_Benchmarks.html"><strong>NurViD</strong></a> (<em>NeurIPS</em>), and <a href="https://link.springer.com/chapter/10.1007/978-3-031-73235-5_27"><strong>OphNet</strong></a> (<em>ECCV</em>).
                  </li>
                  <li>
                    <strong>Trustworthy & Fair AI:</strong> Developing interpretable, generalizable, and fair AI algorithms ensuring reliability in high-stakes clinical applications, as in <a href="https://openaccess.thecvf.com/content/CVPR2023/html/Yan_Towards_Trustable_Skin_Cancer_Diagnosis_via_Rewriting_Models_Decision_CVPR_2023_paper.html"><strong>TrustDerm</strong></a> (<em>CVPR</em>), <a href="https://ieeexplore.ieee.org/abstract/document/10634512"><strong>PLDG</strong></a> (<em>IEEE TMI & MICCAI</em>), and <a href="https://aclanthology.org/2025.emnlp-main.741/"><strong>WISE</strong></a> (<em>EMNLP</em>).
                  </li>
                  <li>
                    <strong>Clinical Translation & Impact:</strong> Driving real-world clinical implementation and validation in oncology and precision medicine through global collobaration, with studies published in <a href="https://www.nature.com/articles/s41746-025-02070-7"><strong><em>npj Digital Medicine</em></strong></a> and <a href="https://academic.oup.com/bjd/advance-article-abstract/doi/10.1093/bjd/ljaf516/8376804"><strong><em>BJD</em></strong></a>.
                  </li>
                </ul>
              </td>
            </tr>
          </tbody>
        </table>

        <table id="news" style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
          <tbody>
            <tr>
              <td style="padding:15px;width:100%;vertical-align:middle">
                <heading>News</heading>
                <ul>
                  <li><strong>[Jan 2026]</strong> PanDerm featured by <a href="https://www.mayoclinicplatform.org/2025/10/14/the-latest-ai-developments-in-dermatology/">Mayo Clinic</a>;  <a href="https://arxiv.org/abs/2512.03445">MAGEN</a> and <a href="https://arxiv.org/abs/2601.01868">DermoGPT</a> released.</li>
                  <li><strong>[Dec 2025]</strong> Our <a href="https://academic.oup.com/bjd/advance-article-abstract/doi/10.1093/bjd/ljaf516/8376804">AI for deep phenotype and risk stratification</a> project published in British Journal of Dermatology (IF: 11.1).</li>
                  <li><strong>[Nov 2025]</strong> Our <a href="https://www.nature.com/articles/s41746-025-02070-7">multi-institutional & global melanoma screening project</a> (ISIC2024) published in npj Digital Medicine (IF: 15.1).</li>
                  <li><strong>[Oct 2025]</strong> Successfully submitted PhD thesis and finished PhD within 3 years at Monash University.</li>
                  <li><strong>[Jun 2025]</strong> <a href="https://www.nature.com/articles/s41591-025-03747-y">PanDerm</a> published in Nature Medicine (IF: 58.7) and featured by editors.</li>
                  <li><strong>[Jun 2025]</strong> <a href="https://openaccess.thecvf.com/content/ICCV2025/html/Yan_Derm1M_A_Million-scale_Vision-Language_Dataset_Aligned_with_Clinical_Ontology_Knowledge_ICCV_2025_paper.html">Derm1M</a> accepted by ICCV (full positive score) and selected as Highlight paper.</li>
                  <li><strong>[May 2025]</strong> <a href="https://link.springer.com/chapter/10.1007/978-3-032-04971-1_35">MAKE</a> early accepted by MICCAI (full positive score).</li>
                </ul>
              </td>
            </tr>
          </tbody>
        </table>

        <table id="publications" style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
          <tbody>
            <tr>
              <td style="padding:15px;width:100%;vertical-align:middle">
                <heading>Publications</heading>
                <p style="font-size:14px;color:#666;">* indicates equal contribution, † indicates corresponding author</p>
                <div style="margin-top:15px;">
<button class="pub-toggle-btn" onclick="togglePublications('selected')">Selected Publications</button>
<button class="pub-toggle-btn active" onclick="togglePublications('all')">By Date</button>
                </div>
              </td>
            </tr>
          </tbody>
        </table>

 <table id="publications-table" style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
  <tbody>
    
    <!-- PanDerm (Nature Medicine) - Selected #1 -->
    <tr class="paper-entry selected" data-year="2025" data-selected-order="1">
 <td style="padding:15px 15px 15px 15px;width:28%;vertical-align:middle">
        <div class="paper-img-container">
          <img src="papers/nm2.png" alt="PanDerm">
        </div>
      </td>
     <td style="padding:15px 15px 15px 20px;width:72%;vertical-align:middle">
        <a href="https://www.nature.com/articles/s41591-025-03747-y" class="paper-title-link"><papertitle style="color:#1d4ed8">A multimodal vision foundation model for clinical dermatology</papertitle></a>
        <br>
        <div data-badge-popover="right" data-badge-type="donut" data-doi="10.1038/s41591-025-03747-y" class="altmetric-embed" style="float:right; margin-left:10px; margin-top:-2px;"></div>
        
        <strong style="color:#000;">Siyuan Yan</strong><span style="color:#666;">, Zhen Yu, Clare Primiero, Cristina Vico-Alonso, Zhonghua Wang, Litao Yang, Philipp Tschandl, Ming Hu, Lie Ju, Gin Tan, Vincent Tang, Aik Beng Ng, David Powell, Paul Bonnington, Simon See, Elisabetta Magnaterra, Peter Ferguson, Jennifer Nguyen, Pascale Guitera, Jose Banuls, Monika Janda, Victoria Mar, Harald Kittler, H Peter Soyer, Zongyuan Ge</span>
        <br>
        <strong>Nature Medicine</strong>, 2025 | <strong>IF: 58.7</strong> | <span style="color:#c41e3a">Editor's Featured</span>
        <br>
        <a href="https://www.nature.com/articles/s41591-025-03747-y">[Paper]</a> / 
        <a href="https://github.com/SiyuanYan1/PanDerm">[Code]</a> / 
        <a href="https://www.mayoclinicplatform.org/2025/10/14/the-latest-ai-developments-in-dermatology/">[Mayo Clinic]</a> / 
        <a href="https://www.usnews.com/news/health-news/articles/2025-06-18/ai-boosts-skin-cancer-diagnoses-study-says">[U.S. News]</a> / 
        <a href="https://www.abc.net.au/listen/programs/nightlife/nightlife-health/105434604">[ABC Nightlife]</a>
      </td>
    </tr>

    <!-- Derm1M (ICCV) - Selected #2 -->
    <tr class="paper-entry selected" data-year="2025" data-selected-order="2">
      <td style="padding:15px 15px 15px 15px;width:22%;vertical-align:middle">
        <div class="paper-img-container">
          <img src="papers/ICCV_Derm1M_poster.png" alt="Derm1M">
        </div>
      </td>
      <td style="padding:15px 15px 15px 20px;width:78%;vertical-align:middle">
        <a href="https://openaccess.thecvf.com/content/ICCV2025/html/Yan_Derm1M_A_Million-scale_Vision-Language_Dataset_Aligned_with_Clinical_Ontology_Knowledge_ICCV_2025_paper.html" class="paper-title-link"><papertitle style="color:#1d4ed8">Derm1M: A million-scale vision-language dataset aligned with clinical ontology knowledge for dermatology</papertitle></a>
        <br>
        <strong style="color:#000;">Siyuan Yan*</strong><span style="color:#666;">, Ming Hu*, Yiwen Jiang*, Xieji Li, Hao Fei, Philipp Tschandl, Harald Kittler, Zongyuan Ge</span>
        <br>
        <strong>ICCV</strong>, 2025 | <span style="color:#c41e3a">Highlight Paper (Top 2.3%)</span>
        <br>
        <a href="https://openaccess.thecvf.com/content/ICCV2025/html/Yan_Derm1M_A_Million-scale_Vision-Language_Dataset_Aligned_with_Clinical_Ontology_Knowledge_ICCV_2025_paper.html">[Paper]</a> / <a href="https://github.com/SiyuanYan1/Derm1M">[Code]</a> / <a href="https://huggingface.co/datasets/redlessone/Derm1M">[Dataset]</a>
      </td>
    </tr>

    <!-- MAKE (MICCAI) - Selected #3 -->
    <tr class="paper-entry selected" data-year="2025" data-selected-order="3">
      <td style="padding:15px 15px 15px 15px;width:22%;vertical-align:middle">
        <div class="paper-img-container">
          <img src="papers/make0.png" alt="MAKE">
        </div>
      </td>
      <td style="padding:15px 15px 15px 20px;width:78%;vertical-align:middle">
        <a href="https://link.springer.com/chapter/10.1007/978-3-032-04971-1_35" class="paper-title-link"><papertitle style="color:#1d4ed8">MAKE: Multi-Aspect Knowledge-Enhanced Vision-Language Pretraining for Zero-shot Dermatological Assessment</papertitle></a>
        <br>
        <strong style="color:#000;">Siyuan Yan*</strong><span style="color:#666;">, Xieji Li*, Ming Hu, Yiwen Jiang, Zhen Yu, Zongyuan Ge</span>
        <br>
        <strong>MICCAI</strong>, 2025 | <span style="color:#c41e3a">Early Accept (Top 9%)</span>
        <br>
        <a href="https://link.springer.com/chapter/10.1007/978-3-032-04971-1_35">[Paper]</a> / <a href="https://github.com/SiyuanYan1/MAKE">[Code]</a>
      </td>
    </tr>

    <!-- TrustDerm (CVPR) - Selected #4 -->
    <tr class="paper-entry selected" data-year="2023" data-selected-order="4">
      <td style="padding:15px 15px 15px 15px;width:22%;vertical-align:middle">
        <div class="paper-img-container">
          <img src="papers/cvpr2.png" alt="TrustDerm">
        </div>
      </td>
      <td style="padding:15px 15px 15px 20px;width:78%;vertical-align:middle">
        <a href="https://openaccess.thecvf.com/content/CVPR2023/html/Yan_Towards_Trustable_Skin_Cancer_Diagnosis_via_Rewriting_Models_Decision_CVPR_2023_paper.html" class="paper-title-link"><papertitle style="color:#1d4ed8">Towards Trustable Skin Cancer Diagnosis via Rewriting Model's Decision</papertitle></a>
        <br>
        <strong style="color:#000;">Siyuan Yan</strong><span style="color:#666;">, Zhen Yu, Xuelin Zhang, Dwarikanath Mahapatra, Shekhar S. Chandra, Monika Janda, H. Peter Soyer, Zongyuan Ge</span>
        <br>
        <strong>CVPR</strong>, 2023
        <br>
        <a href="https://openaccess.thecvf.com/content/CVPR2023/html/Yan_Towards_Trustable_Skin_Cancer_Diagnosis_via_Rewriting_Models_Decision_CVPR_2023_paper.html">[Paper]</a>
      </td>
    </tr>

    <!-- PLDG (IEEE TMI) - Selected #5 -->
    <tr class="paper-entry selected" data-year="2024" data-selected-order="5">
      <td style="padding:15px 10px 15px 15px;width:28%;vertical-align:middle"
        <div class="paper-img-container">
          <img src="papers/TMI.png" alt="PLDG">
        </div>
      </td>
      <td style="padding:15px 15px 15px 20px;width:78%;vertical-align:middle">
        <a href="https://ieeexplore.ieee.org/abstract/document/10634512/" class="paper-title-link"><papertitle style="color:#1d4ed8">Prompt-driven latent domain generalization for medical image classification</papertitle></a>
        <br>
        <strong style="color:#000;">Siyuan Yan</strong><span style="color:#666;">, Zhen Yu, Chi Liu, Lie Ju, Dwarikanath Mahapatra, Brigid Betz-Stablein, Victoria Mar, Monika Janda, Peter Soyer, Zongyuan Ge</span>
        <br>
        <strong>IEEE Transactions on Medical Imaging (TMI)</strong>, 2024 | <strong>IF: 10.6</strong>
        <br>
        <a href="https://ieeexplore.ieee.org/abstract/document/10634512/">[Paper]</a> / <a href="https://github.com/SiyuanYan1/PLDG">[Code]</a>
      </td>
    </tr>

    <!-- npj Digital Medicine - Selected #6 -->
    <tr class="paper-entry selected" data-year="2025" data-selected-order="6">
      <td style="padding:15px 15px 15px 15px;width:22%;vertical-align:middle">
        <div class="paper-img-container">
          <img src="papers/npj1.jpeg" alt="npj">
        </div>
      </td>
      <td style="padding:15px 15px 15px 20px;width:78%;vertical-align:middle">
        <a href="https://www.nature.com/articles/s41746-025-02070-7" class="paper-title-link"><papertitle style="color:#1d4ed8">Automated triage of cancer-suspicious skin lesions with 3D total-body photography</papertitle></a>
        <br>
        <span style="color:#666;">Nicholas R Kurtansky, Maura C Gillis, Noel CF Codella, Brian M D'Alessandro, Zongyuan Ge, Pascale Guitera, Allan C Halpern, Harald Kittler, Josep Malvehy, Konstantinos Liopyris, Victoria J Mar, Linda K Martin, Lara Valeska Maul, Alexander Navarini, Tarlia Rajeswaran, Vin Rajeswaran, Nadia Reichman, H Peter Soyer, Jochen Weber, </span><strong style="color:#000;">Siyuan Yan</strong><span style="color:#666;">, Veronica Rotemberg, Kivanc Kose</span>
        <br>
        <strong>npj Digital Medicine</strong>, 2025 | <strong>IF: 15.1</strong>
        <br>
        <a href="https://www.nature.com/articles/s41746-025-02070-7">[Paper]</a> /
            <a href="https://challenge2024.isic-archive.com/">[ISIC 2024]</a> /
        <a href="https://www.dermatologyrepublic.com.au/total-body-photography-for-melanoma-detection-misses-the-target/1819">[Dermatology Republic]</a> / 
        <a href="https://oncodaily.com/oncolibrary/total-body-photography">[OncoDaily]</a>
      </td>
    </tr>

    <!-- BJD Paper - Selected #7 -->
    <tr class="paper-entry selected" data-year="2025" data-selected-order="7">
      <td style="padding:15px 15px 15px 15px;width:22%;vertical-align:middle">
        <div class="paper-img-container">
          <img src="papers/bjd.png" alt="BJD">
        </div>
      </td>
      <td style="padding:15px 15px 15px 20px;width:78%;vertical-align:middle">
        <a href="https://academic.oup.com/bjd/advance-article-abstract/doi/10.1093/bjd/ljaf516/8376804" class="paper-title-link"><papertitle style="color:#1d4ed8">Automated classification of site-specific cutaneous photodamage using a convolutional neural network and 3D total body photography</papertitle></a>
        <br>
        <span style="color:#666;">Sam Kahler, </span><strong style="color:#000;">Siyuan Yan</strong><span style="color:#666;">, Adam Mothershaw, Francesco Leo, Chantal Rutjes, Zhen Yu, Dilki Jayasinghe, Victoria Mar, Monika Janda, Zongyuan Ge, H Peter Soyer, Brigid Betz-Stablein, Clare Primiero</span>
        <br>
        <strong>British Journal of Dermatology</strong>, 2025 | <strong>IF: 11.1</strong>
        <br>
        <a href="https://academic.oup.com/bjd/advance-article-abstract/doi/10.1093/bjd/ljaf516/8376804">[Paper]</a>
      </td>
    </tr>

    <!-- DermoGPT - Not selected -->
    <tr class="paper-entry" data-year="2026">
      <td style="padding:15px 15px 15px 15px;width:22%;vertical-align:middle">
        <div class="paper-img-container">
          <img src="papers/dermogpt2.png" alt="DermoGPT">
        </div>
      </td>
      <td style="padding:15px 15px 15px 20px;width:78%;vertical-align:middle">
        <a href="https://arxiv.org/abs/2601.01868" class="paper-title-link"><papertitle style="color:#1d4ed8">DermoGPT: Open Weights and Open Data for Morphology-Grounded Dermatological Reasoning MLLMs</papertitle></a>
        <br>
        <span style="color:#666;">Jinghan Ru*, </span><strong style="color:#000;">Siyuan Yan*</strong><span style="color:#666;">, Yuguo Yin, Yuexian Zou, Zongyuan Ge</span>
        <br>
        <strong>Preprint</strong>, 2026
        <br>
        <a href="https://arxiv.org/abs/2601.01868">[Paper]</a> / <a href="https://github.com/mendicant04/DermoGPT">[Code]</a> / <a href="#">[Dataset]</a>
      </td>
    </tr>

    <!-- MAGEN - Not selected -->
    <tr class="paper-entry" data-year="2026">
      <td style="padding:15px 15px 15px 15px;width:22%;vertical-align:middle">
        <div class="paper-img-container">
          <img src="papers/omake.png" alt="MAGEN">
        </div>
      </td>
      <td style="padding:15px 15px 15px 20px;width:78%;vertical-align:middle">
        <a href="https://arxiv.org/abs/2512.03445" class="paper-title-link"><papertitle style="color:#1d4ed8">Multi-Aspect Knowledge-Enhanced Medical Vision-Language Pretraining with Multi-Agent Data Generation</papertitle></a>
        <br>
        <span style="color:#666;">Xieji Li, </span><strong style="color:#000;">Siyuan Yan†</strong><span style="color:#666;">, Yingsheng Liu, H Peter Soyer, Monika Janda, Victoria Mar, Zongyuan Ge</span>
        <br>
        <strong>Preprint</strong>, 2026
        <br>
        <a href="https://arxiv.org/abs/2512.03445">[Paper]</a> / <a href="https://github.com/XiejiLi/MAGEN-O-MAKE">[Code]</a>
      </td>
    </tr>

    <!-- NurViD (NeurIPS) - Not selected -->
    <tr class="paper-entry" data-year="2023">
      <td style="padding:15px 15px 15px 15px;width:22%;vertical-align:middle">
        <div class="paper-img-container">
          <img src="papers/NeurIPS2023.png" alt="NurViD">
        </div>
      </td>
      <td style="padding:15px 15px 15px 20px;width:78%;vertical-align:middle">
        <a href="https://proceedings.neurips.cc/paper_files/paper/2023/file/3a48b0eaba26ba862220a307a9edb0bb-Paper-Datasets_and_Benchmarks.pdf" class="paper-title-link"><papertitle style="color:#1d4ed8">NurViD: A Large Expert-Level Video Database for Nursing Procedure Activity Understanding</papertitle></a>
        <br>
        <span style="color:#666;">Ming Hu*, Lin Wang*, </span><strong style="color:#000;">Siyuan Yan*</strong><span style="color:#666;">, Don Ma, Qingli Ren, Peng Xia, Wei Feng, Peibo Duan, Lie Ju, Zongyuan Ge</span>
        <br>
        <strong>NeurIPS</strong>, 2023
        <br>
        <a href="https://proceedings.neurips.cc/paper_files/paper/2023/file/3a48b0eaba26ba862220a307a9edb0bb-Paper-Datasets_and_Benchmarks.pdf">[Paper]</a> / <a href="https://github.com/minghu0830/NurViD-benchmark">[Code]</a>
      </td>
    </tr>

  </tbody>
</table>

        <table id="awards" style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
          <tbody>
            <tr>
              <td style="padding:15px;width:100%;vertical-align:middle">
                <heading>Selected Awards</heading>
                <ul>
                  <li><strong>2024:</strong> <a href="https://australian.museum/get-involved/eureka-prizes/2024-eureka-prize-winners/">Australian Museum Eureka Prize for Excellence in Interdisciplinary Scientific Research</a></li>    
                  <li><strong>2025:</strong> <a href="https://studymelbourne.vic.gov.au/about-study-melbourne/victorian-international-education-awards/2025-student-winners-and-finalists">Victorian International Education Awards - Research</a></li>
                  <li><strong>2025:</strong> <a href="https://mscan.org.au/morgan-mansell-prize/">Morgan Mansell Prize - Victorian Melanoma Researcher of the Year</a></li>
                </ul>
              </td>
            </tr>
          </tbody>
        </table>

  <table id="press" style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
  <tbody>
    <tr>
      <td style="padding:15px;width:100%;vertical-align:middle">
        <heading>Press</heading>
        <ul>
   <li><strong>Multimodal Dermatology Foundation Model (PanDerm)</strong> was featured by over 100 global media outlets, including <a href="https://www.mayoclinicplatform.org/2025/10/14/the-latest-ai-developments-in-dermatology/">Mayo Clinic Platform</a>, 
                    <a href="https://www.usnews.com/news/health-news/articles/2025-06-18/ai-boosts-skin-cancer-diagnoses-study-says">U.S. News</a>, 
                    <a href="https://www.fiercebiotech.com/medtech/ai-powered-dermatology-tool-boosts-accuracy-skin-cancer-diagnoses-11">FierceBiotech</a>, 
                    <a href="https://www.abc.net.au/listen/programs/nightlife/nightlife-health/105434604">ABC Nightlife</a>, 
                    <a href="https://www.healthday.com/healthpro-news/skin-health/ai-model-can-aid-physicians-in-skin-cancer-diagnoses">HealthDay</a>, 
                    <a href="https://www.eurekalert.org/news-releases/1086707">EurekAlert!</a>, 
                    <a href="https://thedermdigest.com/ai-in-action-panderm-boosts-diagnostic-accuracy-via-multimodal-images/">The Derm Digest</a>, as well as leading Chinese outlets such as 
                    <a href="https://view.inews.qq.com/a/20250610A06R4O00">Tencent News</a>, 
                    <a href="https://finance.sina.cn/2025-06-16/detail-infafyah3217570.d.html">Sina Finance</a>, 
                    <a href="http://news.bioon.com/IntelligentRobot">Bioon</a>, 
                    <a href="https://www.medsci.cn/search?q=PanDerm">MedSci</a>, and 
                    <a href="https://health.baidu.com/m/detail/ar_9361017135798245312">Baidu Health</a>, among <a href="https://nature.altmetric.com/details/177851083/news">others</a>.</li>
          </li>
          <li>
            <strong>AI for 3D Total-Body Photography in Melanoma Screening</strong> was covered by 
            <a href="https://www.dermatologyrepublic.com.au/total-body-photography-for-melanoma-detection-misses-the-target/1819">Dermatology Republic</a>, 
            <a href="https://oncodaily.com/oncolibrary/total-body-photography">OncoDaily</a>.
          </li>
        </ul>
        
       <div class="media-logos">
  <div class="media-logo-wrapper">
    <img src="news/mayo1.png" alt="Mayo Clinic" class="media-logo-item">
  </div>
  <div class="media-logo-wrapper">
    <img src="news/usnews.png" alt="U.S. News" class="media-logo-item">
  </div>
  <div class="media-logo-wrapper">
    <img src="news/fierce.png" alt="FierceBiotech" class="media-logo-item">
  </div>
  <div class="media-logo-wrapper">
    <img src="news/ABC-Listen.png" alt="ABC" class="media-logo-item">
  </div>
  <div class="media-logo-wrapper">
    <img src="news/healthday.png" alt="HealthDay" class="media-logo-item">
  </div>
  <div class="media-logo-wrapper">
    <img src="news/Eurek.png" alt="EurekAlert" class="media-logo-item">
  </div>
  <div class="media-logo-wrapper">
    <img src="news/TDD.png" alt="The Derm Digest" class="media-logo-item">
  </div>
  <div class="media-logo-wrapper">
    <img src="news/t4.png" alt="腾讯新闻" class="media-logo-item">
  </div>
  <div class="media-logo-wrapper">
    <img src="news/xinlang.png" alt="新浪财经" class="media-logo-item">
  </div>
  <div class="media-logo-wrapper">
    <img src="news/baidu2.png" alt="百世健康" class="media-logo-item">
  </div>
</div>
      </td>
    </tr>
  </tbody>
</table>

        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
          <tbody>
            <tr>
              <td style="padding:15px;width:100%;vertical-align:middle">
                <heading>Academic Service</heading>
                <p><strong>Journal Reviewer:</strong> Nature Medicine, The Lancet Regional Health–Europe, IEEE TMI, IEEE TIP, Journal of Investigative Dermatology</p>
                <p><strong>Conference Reviewer:</strong> CVPR, ICCV, NeurIPS, ECCV, AAAI, MICCAI</p>
                <p><strong>Challenge Organizer/Co-Organizer:</strong></p>
                <ul>
                  <li><a href="https://flare-medfm.github.io/">MICCAI 2025 - FLARE Challenge</a></li>
                  <li><a href="https://ophnet-challenge.github.io/">APTOS 2025 - Phase Recognition of Surgical Videos</a></li>
                  <li><a href="https://ophnet-challenge.github.io/">MICCAI 2025 - OMIA Challenge</a></li>
                </ul>
              </td>
            </tr>
          </tbody>
        </table>

<br>
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
          <tbody>
            <tr>
              <td style="padding:0px;width:100%;text-align:center">
                <script type="text/javascript" id="clustrmaps" src="//clustrmaps.com/map_v2.js?d=M97qfQHuZF5EjNcV7Dy9Huc58pzf_lkwg55cVZgbWNs&cl=ffffff&w=300"></script>
              </td>
            </tr>
          </tbody>
        </table>



<script>
function togglePublications(mode) {
  // 更新按钮状态
  document.querySelectorAll('.pub-toggle-btn').forEach(btn => {
    btn.classList.remove('active');
  });
  
  if (mode === 'selected') {
    document.querySelectorAll('.pub-toggle-btn')[0].classList.add('active');
  } else {
    document.querySelectorAll('.pub-toggle-btn')[1].classList.add('active');
  }
  
  const tbody = document.querySelector('#publications-table tbody');
  const allPapers = Array.from(document.querySelectorAll('.paper-entry'));
  
  // 先移除所有show类
  allPapers.forEach(paper => paper.classList.remove('show'));
  
  if (mode === 'selected') {
    // 只显示精选论文
    const selectedPapers = allPapers.filter(p => p.classList.contains('selected'));
    selectedPapers.sort((a, b) => parseInt(a.dataset.selectedOrder) - parseInt(b.dataset.selectedOrder));
    selectedPapers.forEach(paper => {
      tbody.appendChild(paper);
      paper.classList.add('show');
    });
  } else if (mode === 'all') {
    // 显示所有论文
    allPapers.sort((a, b) => parseInt(b.dataset.year) - parseInt(a.dataset.year));
    allPapers.forEach(paper => {
      tbody.appendChild(paper);
      paper.classList.add('show');
    });
  }
}

document.addEventListener('DOMContentLoaded', function() {
  togglePublications('all');
});
</script>

</body>
</html>